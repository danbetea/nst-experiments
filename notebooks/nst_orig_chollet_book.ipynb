{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94b0c22-2f16-4bce-b777-3ed8729344de",
   "metadata": {},
   "source": [
    "#### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9b06a-682d-40c3-bea5-d850bce775e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2abcb-e069-4420-89d3-5615c7e7dc7f",
   "metadata": {},
   "source": [
    "#### Load the base and style images, adjust image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d25771-2e51-4687-8225-4e6966840e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for the images\n",
    "base_img_path = \"../images/san_francisco.jpg\"\n",
    "style_ref_img_path = \"../images/van_gogh_starry_night.jpg\"\n",
    "\n",
    "# load images, adjust sizes so that vertical is 400 pixels\n",
    "original_width, original_height = keras.utils.load_img(base_img_path).size\n",
    "img_height = 400\n",
    "img_width = round(original_width * img_height / original_height) # = 533"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ff2cd-50c0-4896-94e2-4f91b0718be6",
   "metadata": {},
   "source": [
    "#### Utility functions for pre or post processing images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbaa89-7f58-4d76-ae52-1bef486bd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path): \n",
    "    '''\n",
    "    loads and converts the image at image_path to an appropriate array\n",
    "    applies vgg19 preprocessing\n",
    "    '''\n",
    "    img = keras.utils.load_img(image_path, target_size=(img_height, img_width)) \n",
    "    img = keras.utils.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = keras.applications.vgg19.preprocess_input(img) \n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    ''' \n",
    "    converts a numpy array into a valid image\n",
    "    reverts vgg19 preprocessing\n",
    "    '''\n",
    "    img = img.reshape((img_height, img_width, 3))\n",
    "    \n",
    "    # reverses a transformation done by vgg19.preprocess_input\n",
    "    img[:, :, 0] += 103.939\n",
    "    img[:, :, 1] += 116.779\n",
    "    img[:, :, 2] += 123.68\n",
    "    \n",
    "    # BGR to RGB, also part of reverting the transformation\n",
    "    img = img[:, :, ::-1]\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\") \n",
    "    return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e03f903-2945-498b-a1c9-a99251dd5cdc",
   "metadata": {},
   "source": [
    "#### Load the VGG19 model minus the final fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a5499-a8fa-40c0-9e52-9b6b5d031e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19 model loaded with pretrained ImageNet weights\n",
    "# output layers (top 4) not included (as it should be)\n",
    "model = keras.applications.vgg19.VGG19(weights=\"imagenet\", include_top=False) \n",
    "\n",
    "# a dictionary for the activation values of every (target) layer\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# model that returns (in a dict) the activation values of all layers\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9179c8-782f-466e-b704-b884135a1cb7",
   "metadata": {},
   "source": [
    "#### Define the content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0daa4-9681-4bd1-af76-54a116ff425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base_img, combination_img):\n",
    "    '''\n",
    "    computes L2 content loss between base image and the combination (generated) image\n",
    "    '''\n",
    "    return tf.reduce_sum(tf.square(combination_img - base_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a7109-493f-4677-a341-a1246e5fe2d4",
   "metadata": {},
   "source": [
    "#### Define the style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2a868-46e8-48c0-a8dd-826683bfc760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    '''\n",
    "    computes the gram matrix x times x.T, after reshaping appropriately\n",
    "    '''\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1)) \n",
    "    gram = tf.matmul(features, tf.transpose(features)) \n",
    "    return gram\n",
    "\n",
    "def style_loss(style_img, combination_img): \n",
    "    '''\n",
    "    computes the style loss between gram style S and gram combination C matrices (images)\n",
    "    essentially computes\n",
    "                             \\sum_{ij} (S - C)_{ij}^2\n",
    "                            --------------------------\n",
    "                             (2 * channels * size)^2\n",
    "    '''\n",
    "    S = gram_matrix(style_img)\n",
    "    C = gram_matrix(combination_img) \n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b025c8-213b-4bbb-ab93-f92501cca82f",
   "metadata": {},
   "source": [
    "#### Define the total variation (continuity) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d373ce-0481-4a73-a5fd-df8bac881db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    '''\n",
    "    total variation loss ensures continuity across resulting image\n",
    "    as an L 1.25 norm\n",
    "    in both the vertical (a tensor) and horizontal (b tensor) direction\n",
    "    '''\n",
    "    a = tf.square(x[:, : img_height - 1, : img_width - 1, :] - x[:, 1:, : img_width - 1, :])\n",
    "    b = tf.square(x[:, : img_height - 1, : img_width - 1, :] - x[:, : img_height - 1, 1:, :])\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f277c010-cf57-410a-9c5d-473bce4f76aa",
   "metadata": {},
   "source": [
    "#### Define the total loss, set loss weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34e29a-8d82-4479-be1a-116a58dee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers to use for the style loss\n",
    "style_layer_names = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "] \n",
    "\n",
    "# layer to use for the content loss\n",
    "content_layer_name = \"block5_conv2\"\n",
    "\n",
    "# weight for total variation/style/content loss\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "def compute_loss(combination_image, base_image, style_reference_image): \n",
    "    '''\n",
    "    computes the total loss, by adding: content + style + total_variation\n",
    "    '''\n",
    "    input_tensor = tf.concat([base_image, style_reference_image, combination_image], axis=0)\n",
    "    \n",
    "    # extract weights\n",
    "    features = feature_extractor(input_tensor)\n",
    "    \n",
    "    # initialize loss\n",
    "    loss = tf.zeros(shape=())\n",
    "    \n",
    "    # add content loss\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss = loss + content_weight * content_loss(base_image_features, combination_features)\n",
    "    \n",
    "    # add style loss\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name] \n",
    "        style_reference_features = layer_features[1, :, :, :] \n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        style_loss_value = style_loss(style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layer_names)) * style_loss_value \n",
    "        \n",
    "    # add total variation loss\n",
    "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d210e4-6d65-4682-8e3d-1678c366e73c",
   "metadata": {},
   "source": [
    "#### Compute the loss and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa28ff9-9e57-4af4-ae93-d2ea810a8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # makes training step fast by compiling as a tf.function\n",
    "def compute_loss_and_grads(combination_image, base_image, style_reference_image): \n",
    "    '''\n",
    "    computes loss and gradients\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, base_image, style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image) \n",
    "    \n",
    "    return loss, grads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1be6d365-d746-41b8-bded-9f45ff5eacf8",
   "metadata": {},
   "source": [
    "#### Load SGD with momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0d0a5-898d-4dc9-bff9-a715d70e34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SGD optimizer (gradient descent with momentum)\n",
    "# start with a learning rate of 100 and decrease it by 4% every 100 steps\n",
    "# TODO: lots of optimization (pun intended) could go in here\n",
    "optimizer = keras.optimizers.SGD(keras.optimizers.schedules.ExponentialDecay(\n",
    "     initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06848c3b-bb21-49ca-aeea-22fcd4f14465",
   "metadata": {},
   "source": [
    "#### Preprocess images before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240169c8-6e73-4093-941b-2c61d9b0de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess original and style image\n",
    "base_img = preprocess_image(base_img_path)\n",
    "style_ref_img = preprocess_image(style_ref_img_path)\n",
    "\n",
    "# will be updated throughout, so use a Variable to store it\n",
    "combination_img = tf.Variable(preprocess_image(base_img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292208a-d0ff-483f-916d-dce0becc94c6",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79409f77-87f1-4d69-b886-208b49b96950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iterations = 4000\n",
    "for i in range(1, iterations + 1):    \n",
    "    tic = time()\n",
    "    # compute loss and gradients\n",
    "    loss, grads = compute_loss_and_grads(combination_img, base_img, style_ref_img)\n",
    "    # do gradient descent\n",
    "    # update the combination image in a direction minimizing the loss\n",
    "    optimizer.apply_gradients([(grads, combination_img)]) \n",
    "    toc = time()\n",
    "\n",
    "    # comment the below line if you don't want step by step details \n",
    "    print(f\"iteration {i:04}: loss={loss:.4f} time={(toc - tic):02.2f}\")\n",
    "    \n",
    "    # save img at regular intervals\n",
    "    if i % 100 == 0:\n",
    "        img = deprocess_image(combination_img.numpy()) \n",
    "        fname = f\"../images/generated_image_at_iteration_{i}.png\" \n",
    "        keras.utils.save_img(fname, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d217b-5978-438f-91cc-750055651268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
